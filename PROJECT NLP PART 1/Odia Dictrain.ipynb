{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c487be83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d140612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_odia_text(text):\n",
    "  \"\"\"Preprocesses Odia text by lowercasing, removing punctuation, and tokenizing it.\"\"\"\n",
    "\n",
    "  text = text.lower()\n",
    "  text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "  text = text.split()\n",
    "\n",
    "  return text\n",
    "\n",
    "\n",
    "with open(\"./DATASET/Odia_Dataset.json\") as f:\n",
    "  data = json.load(f)\n",
    "\n",
    "for text in data:\n",
    "  text = preprocess_odia_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b048313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive', 'neutral', 'neutral']\n"
     ]
    }
   ],
   "source": [
    "def sentiment_label(text):\n",
    "  \"\"\"Labels the sentiment of a text string as positive, negative, or neutral.\"\"\"\n",
    "\n",
    "  if text.startswith(\"I love\"):\n",
    "    return \"positive\"\n",
    "  elif text.startswith(\"I hate\"):\n",
    "    return \"negative\"\n",
    "  else:\n",
    "    return \"neutral\"\n",
    "\n",
    "\n",
    "texts = [\"I love this movie!\", \"This movie is terrible!\", \"I thought the movie was okay.\"]\n",
    "\n",
    "sentiment_labels = []\n",
    "\n",
    "for text in texts:\n",
    "  sentiment_labels.append(sentiment_label(text))\n",
    "\n",
    "print(sentiment_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b35227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4025327f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         èହ  ñଆସ   ñସ   सر   सل   નથ  ଅଇଚ  ଅଇଜ  ଅଇଟ  ଅଇଷ  ...   ୱସ  ୱସନ  ୱସପ  \\\n",
      "0       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "1       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "2       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "3       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "4       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "...     ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "208041  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "208042  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "208043  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "208044  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "208045  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "\n",
      "        ୱସମ   ୱହ   ୱୟ  ୱୟନ  ୱୟର   ୱୱ  അവକ  \n",
      "0       0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1       0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2       0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3       0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4       0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "...     ...  ...  ...  ...  ...  ...  ...  \n",
      "208041  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "208042  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "208043  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "208044  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "208045  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[208046 rows x 6845 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the JSON \n",
    "with open(\"./DATASET/Odia_Dataset.json\", 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract all the values (Odia text) \n",
    "text_features = list(data.values())\n",
    "\n",
    "# TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# vectorizer fit check\n",
    "vectorizer.fit(text_features)\n",
    "\n",
    "#  TF-IDF features are being transfered\n",
    "tf_idf_features = vectorizer.transform(text_features)\n",
    "\n",
    "#  TF-IDF features to a Pandas DataFrame conversion:\n",
    "tf_idf_df = pd.DataFrame(tf_idf_features.toarray(), columns=vectorizer.get_feature_names_out())\n",
    " \n",
    "print(tf_idf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12b832b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        160  360  8230   _a   _k   _l   _m   _o   _p   _u  ...   ୱସ  ୱସନ  ୱସପ  \\\n",
      "0       0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "1       0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "2       0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "3       0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "4       0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "...     ...  ...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "208172  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "208173  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "208174  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "208175  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "208176  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "\n",
      "        ୱସମ   ୱହ   ୱୟ  ୱୟନ  ୱୟର   ୱୱ  അവକ  \n",
      "0       0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1       0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2       0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3       0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4       0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "...     ...  ...  ...  ...  ...  ...  ...  \n",
      "208172  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "208173  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "208174  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "208175  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "208176  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[208177 rows x 6949 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the JSON \n",
    "with open(\"./DATASET/Odia_Dataset1.json\", 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract all the values (Odia text) \n",
    "text_features = list(data.values())\n",
    "\n",
    "# TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# vectorizer fit check\n",
    "vectorizer.fit(text_features)\n",
    "\n",
    "#  TF-IDF features are being transfered\n",
    "tf_idf_features = vectorizer.transform(text_features)\n",
    "\n",
    "#  TF-IDF features to a Pandas DataFrame conversion:\n",
    "tf_idf_df = pd.DataFrame(tf_idf_features.toarray(), columns=vectorizer.get_feature_names_out())\n",
    " \n",
    "print(tf_idf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8db63b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         11  1337   16   18   1c  2019   22   24   25   40  ...   ୱସ  ୱସନ  \\\n",
      "0       0.0   0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "1       0.0   0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "2       0.0   0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "3       0.0   0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "4       0.0   0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "...     ...   ...  ...  ...  ...   ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "215334  0.0   0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "215335  0.0   0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "215336  0.0   0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "215337  0.0   0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "215338  0.0   0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "\n",
      "        ୱସପ  ୱସମ   ୱହ   ୱୟ  ୱୟନ  ୱୟର   ୱୱ  അവକ  \n",
      "0       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "...     ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "215334  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "215335  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "215336  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "215337  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "215338  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[215339 rows x 9538 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the JSON \n",
    "with open(\"./DATASET/Odia.Dataset3.json\", 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract all the values (Odia text) \n",
    "text_features = list(data.values())\n",
    "\n",
    "# TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# vectorizer fit check\n",
    "vectorizer.fit(text_features)\n",
    "\n",
    "#  TF-IDF features are being transfered\n",
    "tf_idf_features = vectorizer.transform(text_features)\n",
    "\n",
    "#  TF-IDF features to a Pandas DataFrame conversion:\n",
    "tf_idf_df = pd.DataFrame(tf_idf_features.toarray(), columns=vectorizer.get_feature_names_out())\n",
    " \n",
    "print(tf_idf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dae8cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6502/6502 [==============================] - 71s 11ms/step - loss: 0.0047 - accuracy: 0.9999\n",
      "Epoch 2/100\n",
      "6502/6502 [==============================] - 54s 8ms/step - loss: 3.0680e-07 - accuracy: 1.0000\n",
      "Epoch 3/100\n",
      "6502/6502 [==============================] - 48s 7ms/step - loss: 1.0777e-08 - accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "6502/6502 [==============================] - 48s 7ms/step - loss: 6.9160e-10 - accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "6502/6502 [==============================] - 57s 9ms/step - loss: 2.2862e-10 - accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "6502/6502 [==============================] - 50s 8ms/step - loss: 1.0772e-10 - accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "6502/6502 [==============================] - 62s 9ms/step - loss: 5.3289e-11 - accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "6502/6502 [==============================] - 48s 7ms/step - loss: 3.0369e-11 - accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "6502/6502 [==============================] - 48s 7ms/step - loss: 3.8964e-11 - accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "6502/6502 [==============================] - 47s 7ms/step - loss: 2.8650e-11 - accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "6502/6502 [==============================] - 47s 7ms/step - loss: 2.2347e-11 - accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "6502/6502 [==============================] - 64s 10ms/step - loss: 1.3752e-11 - accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "6502/6502 [==============================] - 53s 8ms/step - loss: 1.1460e-11 - accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "6502/6502 [==============================] - 50s 8ms/step - loss: 1.1460e-11 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "6502/6502 [==============================] - 49s 8ms/step - loss: 1.3752e-11 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "6502/6502 [==============================] - 49s 7ms/step - loss: 8.0219e-12 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "6502/6502 [==============================] - 49s 7ms/step - loss: 8.5949e-12 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "6502/6502 [==============================] - 49s 7ms/step - loss: 1.2606e-11 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "6502/6502 [==============================] - 53s 8ms/step - loss: 9.7409e-12 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "6502/6502 [==============================] - 50s 8ms/step - loss: 6.8759e-12 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "6502/6502 [==============================] - 51s 8ms/step - loss: 4.0110e-12 - accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "6502/6502 [==============================] - 52s 8ms/step - loss: 4.5840e-12 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "6502/6502 [==============================] - 50s 8ms/step - loss: 8.5949e-12 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "6502/6502 [==============================] - 49s 8ms/step - loss: 3.4380e-12 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "6502/6502 [==============================] - 50s 8ms/step - loss: 3.4380e-12 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "6502/6502 [==============================] - 48s 7ms/step - loss: 4.0110e-12 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "6502/6502 [==============================] - 49s 8ms/step - loss: 1.7190e-12 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "6502/6502 [==============================] - 49s 8ms/step - loss: 4.0110e-12 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "6502/6502 [==============================] - 50s 8ms/step - loss: 2.8650e-12 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "6502/6502 [==============================] - 49s 7ms/step - loss: 3.4380e-12 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "6502/6502 [==============================] - 51s 8ms/step - loss: 2.8650e-12 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "6502/6502 [==============================] - 51s 8ms/step - loss: 2.8650e-12 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "6502/6502 [==============================] - 49s 8ms/step - loss: 2.2920e-12 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "6502/6502 [==============================] - 51s 8ms/step - loss: 2.2920e-12 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "6502/6502 [==============================] - 51s 8ms/step - loss: 1.7190e-12 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "6502/6502 [==============================] - 50s 8ms/step - loss: 1.7190e-12 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "6502/6502 [==============================] - 49s 8ms/step - loss: 1.1460e-12 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "6502/6502 [==============================] - 50s 8ms/step - loss: 1.1460e-12 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "6502/6502 [==============================] - 50s 8ms/step - loss: 2.8650e-12 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "6502/6502 [==============================] - 49s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "6502/6502 [==============================] - 50s 8ms/step - loss: 5.7299e-13 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "6502/6502 [==============================] - 51s 8ms/step - loss: 5.7299e-13 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "6502/6502 [==============================] - 55s 8ms/step - loss: 1.1460e-12 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "6502/6502 [==============================] - 54s 8ms/step - loss: 5.7299e-13 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "6502/6502 [==============================] - 54s 8ms/step - loss: 5.7299e-13 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "6502/6502 [==============================] - 48s 7ms/step - loss: 1.1460e-12 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "6502/6502 [==============================] - 49s 8ms/step - loss: 1.7190e-12 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "6502/6502 [==============================] - 48s 7ms/step - loss: 1.1460e-12 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "6502/6502 [==============================] - 50s 8ms/step - loss: 5.7299e-13 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "6502/6502 [==============================] - 49s 8ms/step - loss: 5.7299e-13 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "6502/6502 [==============================] - 49s 8ms/step - loss: 1.1460e-12 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "6502/6502 [==============================] - 56s 9ms/step - loss: 5.7299e-13 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "6502/6502 [==============================] - 62s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "6502/6502 [==============================] - 49s 8ms/step - loss: 5.7299e-13 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "6502/6502 [==============================] - 50s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "6502/6502 [==============================] - 48s 7ms/step - loss: 1.1460e-12 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "6502/6502 [==============================] - 48s 7ms/step - loss: 5.7299e-13 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "6502/6502 [==============================] - 48s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "6502/6502 [==============================] - 49s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "6502/6502 [==============================] - 48s 7ms/step - loss: 5.7299e-13 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "6502/6502 [==============================] - 51s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "6502/6502 [==============================] - 49s 8ms/step - loss: 5.7299e-13 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "6502/6502 [==============================] - 49s 8ms/step - loss: 5.7299e-13 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "6502/6502 [==============================] - 49s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "6502/6502 [==============================] - 50s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "6502/6502 [==============================] - 48s 7ms/step - loss: 1.1460e-12 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "6502/6502 [==============================] - 52s 8ms/step - loss: 5.7299e-13 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "6502/6502 [==============================] - 54s 8ms/step - loss: 5.7299e-13 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "6502/6502 [==============================] - 53s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "6502/6502 [==============================] - 47s 7ms/step - loss: 1.1460e-12 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "6502/6502 [==============================] - 49s 8ms/step - loss: 5.7299e-13 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "6502/6502 [==============================] - 51s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "6502/6502 [==============================] - 50s 8ms/step - loss: 5.7299e-13 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "6502/6502 [==============================] - 51s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "6502/6502 [==============================] - 48s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "6502/6502 [==============================] - 55s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "6502/6502 [==============================] - 59s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "6502/6502 [==============================] - 50s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "6502/6502 [==============================] - 52s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "6502/6502 [==============================] - 56s 9ms/step - loss: 5.7299e-13 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "6502/6502 [==============================] - 52s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "6502/6502 [==============================] - 50s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "6502/6502 [==============================] - 50s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "6502/6502 [==============================] - 53s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "6502/6502 [==============================] - 51s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "6502/6502 [==============================] - 52s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "6502/6502 [==============================] - 49s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "6502/6502 [==============================] - 49s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "6502/6502 [==============================] - 48s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "6502/6502 [==============================] - 50s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "6502/6502 [==============================] - 48s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "6502/6502 [==============================] - 49s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "6502/6502 [==============================] - 88s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "6502/6502 [==============================] - 52s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "6502/6502 [==============================] - 49s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "6502/6502 [==============================] - 50s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "6502/6502 [==============================] - 54s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "6502/6502 [==============================] - 50s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "6502/6502 [==============================] - 48s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "6502/6502 [==============================] - 243s 37ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2c4569090>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the JSON file\n",
    "with open(\"./DATASET/Odia_Dataset.json\", 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the Odia phrases as your input data\n",
    "odia_data = list(data.values())\n",
    "\n",
    "# Create labels for your data (assuming all phrases have neutral sentiment)\n",
    "labels = [\"neutral\"] * len(odia_data)\n",
    "\n",
    "# Tokenize the input data (convert text to numerical sequences)\n",
    "tokenizer = tf.keras.layers.TextVectorization(max_tokens=10000)  # You can adjust max_tokens as needed\n",
    "tokenizer.adapt(odia_data)\n",
    "\n",
    "# Create the LSTM model\n",
    "model = tf.keras.Sequential([\n",
    "    tokenizer,\n",
    "    tf.keras.layers.Embedding(input_dim=len(tokenizer.get_vocabulary()), output_dim=128),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(3, activation=\"softmax\")  # Assuming three classes: positive, negative, neutral\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Tokenize the labels (convert labels to numerical format)\n",
    "label_tokenizer = tf.keras.layers.TextVectorization(max_tokens=3)  # Three classes: positive, negative, neutral\n",
    "label_tokenizer.adapt(labels)\n",
    "\n",
    "# Convert labels to numerical format\n",
    "numerical_labels = np.array(label_tokenizer(labels))\n",
    "\n",
    "# Train the model\n",
    "model.fit(np.array(odia_data), numerical_labels, epochs=100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8040ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6506/6506 [==============================] - 63s 9ms/step - loss: 0.0050 - accuracy: 0.9998\n",
      "Epoch 2/100\n",
      "6506/6506 [==============================] - 56s 9ms/step - loss: 4.3882e-07 - accuracy: 1.0000\n",
      "Epoch 3/100\n",
      "6506/6506 [==============================] - 54s 8ms/step - loss: 1.6415e-08 - accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "6506/6506 [==============================] - 54s 8ms/step - loss: 1.1790e-09 - accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "6506/6506 [==============================] - 54s 8ms/step - loss: 1.9527e-10 - accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 8.6468e-11 - accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "6506/6506 [==============================] - 49s 7ms/step - loss: 3.8366e-11 - accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "6506/6506 [==============================] - 48s 7ms/step - loss: 3.0350e-11 - accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "6506/6506 [==============================] - 49s 8ms/step - loss: 3.6076e-11 - accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "6506/6506 [==============================] - 48s 7ms/step - loss: 3.4358e-11 - accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "6506/6506 [==============================] - 53s 8ms/step - loss: 1.8897e-11 - accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "6506/6506 [==============================] - 54s 8ms/step - loss: 1.7752e-11 - accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "6506/6506 [==============================] - 56s 9ms/step - loss: 1.0307e-11 - accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "6506/6506 [==============================] - 60s 9ms/step - loss: 4.5811e-12 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "6506/6506 [==============================] - 58s 9ms/step - loss: 3.4358e-12 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 9.1621e-12 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "6506/6506 [==============================] - 52s 8ms/step - loss: 8.0169e-12 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "6506/6506 [==============================] - 52s 8ms/step - loss: 8.0169e-12 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "6506/6506 [==============================] - 53s 8ms/step - loss: 4.0084e-12 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "6506/6506 [==============================] - 57s 9ms/step - loss: 4.5811e-12 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 5.7263e-12 - accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "6506/6506 [==============================] - 55s 8ms/step - loss: 5.7263e-12 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "6506/6506 [==============================] - 69s 11ms/step - loss: 8.5895e-12 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "6506/6506 [==============================] - 53s 8ms/step - loss: 4.0084e-12 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "6506/6506 [==============================] - 57s 9ms/step - loss: 2.8632e-12 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "6506/6506 [==============================] - 59s 9ms/step - loss: 6.2990e-12 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "6506/6506 [==============================] - 60s 9ms/step - loss: 1.1453e-12 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 4.0084e-12 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "6506/6506 [==============================] - 57s 9ms/step - loss: 4.0084e-12 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "6506/6506 [==============================] - 49s 8ms/step - loss: 1.1453e-12 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "6506/6506 [==============================] - 49s 7ms/step - loss: 2.8632e-12 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "6506/6506 [==============================] - 49s 8ms/step - loss: 3.4358e-12 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "6506/6506 [==============================] - 61s 9ms/step - loss: 5.7263e-13 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "6506/6506 [==============================] - 63s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "6506/6506 [==============================] - 55s 8ms/step - loss: 1.1453e-12 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "6506/6506 [==============================] - 47s 7ms/step - loss: 1.1453e-12 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "6506/6506 [==============================] - 71s 11ms/step - loss: 1.7179e-12 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "6506/6506 [==============================] - 62s 10ms/step - loss: 5.7263e-13 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "6506/6506 [==============================] - 64s 10ms/step - loss: 1.7179e-12 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "6506/6506 [==============================] - 62s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "6506/6506 [==============================] - 73s 11ms/step - loss: 5.7263e-13 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "6506/6506 [==============================] - 56s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "6506/6506 [==============================] - 56s 9ms/step - loss: 1.7179e-12 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "6506/6506 [==============================] - 55s 8ms/step - loss: 1.7179e-12 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "6506/6506 [==============================] - 48s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "6506/6506 [==============================] - 68s 10ms/step - loss: 5.7263e-13 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "6506/6506 [==============================] - 53s 8ms/step - loss: 1.1453e-12 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "6506/6506 [==============================] - 56s 9ms/step - loss: 1.1453e-12 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "6506/6506 [==============================] - 61s 9ms/step - loss: 5.7263e-13 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "6506/6506 [==============================] - 56s 9ms/step - loss: 5.7263e-13 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "6506/6506 [==============================] - 57s 9ms/step - loss: 5.7263e-13 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "6506/6506 [==============================] - 53s 8ms/step - loss: 5.7263e-13 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 5.7263e-13 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "6506/6506 [==============================] - 50s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "6506/6506 [==============================] - 52s 8ms/step - loss: 1.1453e-12 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "6506/6506 [==============================] - 52s 8ms/step - loss: 5.7263e-13 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 5.7263e-13 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 5.7263e-13 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "6506/6506 [==============================] - 53s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "6506/6506 [==============================] - 52s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "6506/6506 [==============================] - 56s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "6506/6506 [==============================] - 53s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "6506/6506 [==============================] - 52s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "6506/6506 [==============================] - 55s 8ms/step - loss: 5.7263e-13 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "6506/6506 [==============================] - 52s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "6506/6506 [==============================] - 53s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "6506/6506 [==============================] - 52s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "6506/6506 [==============================] - 53s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "6506/6506 [==============================] - 52s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "6506/6506 [==============================] - 56s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "6506/6506 [==============================] - 55s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "6506/6506 [==============================] - 57s 9ms/step - loss: 5.7263e-13 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "6506/6506 [==============================] - 53s 8ms/step - loss: 5.7263e-13 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "6506/6506 [==============================] - 54s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "6506/6506 [==============================] - 56s 9ms/step - loss: 5.7263e-13 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "6506/6506 [==============================] - 53s 8ms/step - loss: 5.7263e-13 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "6506/6506 [==============================] - 54s 8ms/step - loss: 4.8131e-07 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "6506/6506 [==============================] - 52s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "6506/6506 [==============================] - 50s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "6506/6506 [==============================] - 54s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "6506/6506 [==============================] - 52s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "6506/6506 [==============================] - 50s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "6506/6506 [==============================] - 50s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "6506/6506 [==============================] - 57s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "6506/6506 [==============================] - 53s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "6506/6506 [==============================] - 54s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "6506/6506 [==============================] - 52s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "6506/6506 [==============================] - 52s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "6506/6506 [==============================] - 52s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "6506/6506 [==============================] - 52s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "6506/6506 [==============================] - 53s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "6506/6506 [==============================] - 52s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "6506/6506 [==============================] - 52s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "6506/6506 [==============================] - 54s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x293b89550>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the JSON file\n",
    "with open(\"./DATASET/Odia_Dataset1.json\", 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the Odia phrases as your input data\n",
    "odia_data = list(data.values())\n",
    "\n",
    "# Create labels for your data (assuming all phrases have neutral sentiment)\n",
    "labels = [\"neutral\"] * len(odia_data)\n",
    "\n",
    "# Tokenize the input data (convert text to numerical sequences)\n",
    "tokenizer = tf.keras.layers.TextVectorization(max_tokens=10000)  # You can adjust max_tokens as needed\n",
    "tokenizer.adapt(odia_data)\n",
    "\n",
    "# Create the LSTM model\n",
    "model = tf.keras.Sequential([\n",
    "    tokenizer,\n",
    "    tf.keras.layers.Embedding(input_dim=len(tokenizer.get_vocabulary()), output_dim=128),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(3, activation=\"softmax\")  # Assuming three classes: positive, negative, neutral\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Tokenize the labels (convert labels to numerical format)\n",
    "label_tokenizer = tf.keras.layers.TextVectorization(max_tokens=3)  # Three classes: positive, negative, neutral\n",
    "label_tokenizer.adapt(labels)\n",
    "\n",
    "# Convert labels to numerical format\n",
    "numerical_labels = np.array(label_tokenizer(labels))\n",
    "\n",
    "# Train the model\n",
    "model.fit(np.array(odia_data), numerical_labels, epochs=100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
